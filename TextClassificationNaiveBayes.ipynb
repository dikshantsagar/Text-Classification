{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "from collections import Counter\n",
    "li=[]\n",
    "# the folder 20_newsgroups is in the same path as the jupyter notebook\n",
    "for infile in glob.glob('data/traindata/*/*'):\n",
    "    \n",
    "    try:\n",
    "        review_file = open(infile,'r').read()\n",
    "            \n",
    "    except UnicodeDecodeError:\n",
    "            print('exception')\n",
    "    li.append(review_file.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(input):\n",
    "    new_list = []\n",
    "    for i in input:\n",
    "        for j in i:\n",
    "            new_list.append(j)\n",
    "    return new_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "li=flatten(li)       # to make a 1d list out of a 2d list containing words via split(' ') from training documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dic=Counter(li)\n",
    "wordlist=[]\n",
    "for i in dic:\n",
    "    wordlist.append([dic[i],i])                    # making a 2d list with unique words with their counts\n",
    "l=len(wordlist)\n",
    "print('len--',l)\n",
    "i=0\n",
    "while i < l:\n",
    "    if wordlist[i][0]<5 or wordlist[i][1]=='':     # removing empty strings and words with count less than  5 \n",
    "        print(i)\n",
    "        wordlist.pop(i)\n",
    "        i-=1\n",
    "        l-=1\n",
    "    i+=1\n",
    "i=0\n",
    "while i < l:\n",
    "    if wordlist[i][1].lower() in stop:           # removing stop words\n",
    "        print(i)\n",
    "        wordlist.pop(i)\n",
    "        i-=1\n",
    "        l-=1\n",
    "    i+=1\n",
    "i=0\n",
    "while i < l:\n",
    "    if wordlist[i][1].isalpha()==False:          # selecting srings that contain only alphabets to get filtered words\n",
    "        print(i)\n",
    "        wordlist.pop(i)\n",
    "        i-=1\n",
    "        l-=1\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2d list with unique filtered words with count >=5\n",
    "wordlist         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterlist=sorted(wordlist,reverse=True)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iterlist)    # sorted 2d list with unique words to select top words for the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary=[]\n",
    "for i in range(6000):                    # forming the vocabulary list by selecting top 6000 words via count\n",
    "    vocabulary.append(iterlist[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary               # vocabulary list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=np.asarray(vocabulary)\n",
    "\n",
    "df=pd.DataFrame(vocab)                         #saving the vocabulary list as .csv file for further use such that data\n",
    "                                                # loading doesnt have to be done again and again\n",
    "df.to_csv('vocab.csv',sep=',',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "with open('vocab.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    vocab = list(reader)\n",
    "vocab=flatten(vocab)                        # reding the vocab.csv file to get the vocabulary list\n",
    "print(len(vocab))\n",
    "col=[]\n",
    "col=vocab[:]\n",
    "col.append('Target')\n",
    "traindf=pd.DataFrame(index= range(20) , columns=np.array(col))    # forming the training dataframe containing the word\n",
    "print(len(col),len(vocab))                                        # count for every word in vocabulary in a particular \n",
    "                                                                  # class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "for i in range(20):\n",
    "    li=[]\n",
    "    trainlist=[]\n",
    "    for infile in glob.glob('data/traindata/'+str(i)+'/*'):  # path: ~/i/* for class i\n",
    "        try:\n",
    "            review_file = open(infile,'r').read()       #reading training files \n",
    "            \n",
    "        except UnicodeDecodeError:\n",
    "            print('exception')\n",
    "            \n",
    "        li.append(review_file.split(\" \"))                    #TRAINING \n",
    "        \n",
    "    li=flatten(li)\n",
    "    wordict=Counter(li)\n",
    "    for word in vocab:\n",
    "        if word in wordict:                             # counting the words present in vocabulary in the files \n",
    "            trainlist.append(wordict[word])             # for a particular class\n",
    "        else:\n",
    "            trainlist.append(0)\n",
    "    trainlist.append(i)\n",
    "    traindf.iloc[i,:]=np.asarray(trainlist)            # appending to the training dataframe\n",
    "    print(i)\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf=pd.DataFrame(index=range(4000),columns=np.array(vocab))\n",
    "testdf.fillna(0,inplace=True)           # creating a testing dataframe for 4000 files\n",
    "testdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for infile in glob.glob('testfilenamed/*'): #path where all the test files are stores together\n",
    "    testli=[]\n",
    "    countlist=[]\n",
    "    \n",
    "    try:\n",
    "        review_file = open(infile,'r').read()           #reading tesfiles\n",
    "            \n",
    "    except UnicodeDecodeError:\n",
    "            print('exception agya')\n",
    "            \n",
    "    \n",
    "    testli.append(review_file.split(\" \"))          # TESTING\n",
    "    print('i---',i)\n",
    "    testli=flatten(testli)\n",
    "\n",
    "    wordcount=Counter(testli)\n",
    "        \n",
    "    for word in vocab:\n",
    "        if word in wordcount:\n",
    "            countlist.append(wordcount[word])  # appending to  a list  storing count of words from the vocabulary in\n",
    "        else:                                   # single test file.\n",
    "            countlist.append(0)\n",
    "    testdf.iloc[i,:]=np.asarray(countlist)     # forming the testing dataframe containing count of vocabulary words in \n",
    "                                              # a single test file for 4000 files.\n",
    "    \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inbuilt Multinomial Naive Bayes for Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf=MultinomialNB()                              #initialising the inbuilt multinomial naive bayes classifier\n",
    "clf.fit(traindf.iloc[:,0:-1],traindf.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylist=[]\n",
    "for infile in glob.glob('testfilenamed/*'):\n",
    "    li=infile.split('/')\n",
    "    var=li[-1].split('-')            # getting the actual classes of the testfiles \n",
    "    ylist.append(var[0])             # from their names(mmaped to a range - 0 to 19 for convenience)\n",
    "    \n",
    "ylist = list(map(int, ylist))\n",
    "ylist\n",
    "ydf=pd.DataFrame(np.array(ylist).reshape(4000,1),columns=['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score with inbulit naive bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(testdf,ydf)                  # testing with the inbuilt naive bayes classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Implimented Multinomial Naive Bayes for Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(testdf,vocab,df):\n",
    "    \n",
    "    probab_list=[]\n",
    "    for k in range(4000):  # for 4000 test files \n",
    "        \n",
    "        print(k)\n",
    "        plist=[]\n",
    "        \n",
    "        for i in range(20):     # checking for every class \n",
    "            \n",
    "            out=0\n",
    "            a=time.time()\n",
    "            \n",
    "            for j in range(6000):       # for every word in the vocabulary\n",
    "                \n",
    "                out+=np.log((testdf.iloc[k,j]+1))-np.log(sum(traindf.iloc[i,:])+6000)  # calculating word log probabilities\n",
    "                \n",
    "            b=time.time()\n",
    "            print('ab',b-a)\n",
    "            \n",
    "            prob=out+np.log(1/20)  #adding log probability of a class\n",
    "            plist.append(prob)\n",
    "              \n",
    "        print(plist)\n",
    "        maxindex=plist.index(max(plist))  # getting the index of the class with the maximum probability\n",
    "        print('pred',maxindex)\n",
    "        probab_list.append(maxindex)\n",
    "    \n",
    "    \n",
    "    print('prob--',prob)\n",
    "    return probab_list        # returning the list with predicted classes  for  4000 test files\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_prob=1/20\n",
    "prediction=[]\n",
    "st=time.time()                                                  # SELF IMPLIMENTED NAIVE BAYES CLASSIFIER\n",
    "problist=probability(testdf,vocab,traindf)                   # being called for predictions from the testing dataframe\n",
    "print(problist)\n",
    "en=time.time()\n",
    "print('time taken - ',en-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(prediction,ylist):\n",
    "    a=np.array(prediction)\n",
    "    b=np.array(ylist)              # score function for the self implimented naive bayes classifier\n",
    "    c=list(a-b)\n",
    "    total=len(c)\n",
    "    r=c.count(0)\n",
    "    print(r/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score with self implimented naive bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score(prediction,ylist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
